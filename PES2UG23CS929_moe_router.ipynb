{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-title",
      "metadata": {
        "id": "cell-title"
      },
      "source": [
        "# Mixture of Experts (MoE) Router â€” Unit 2 Assignment\n",
        "\n",
        "**Author:** Sourabh S Hegde  \n",
        "**SRN:** PES2UG23CS929  \n",
        "**Class:** CSE 6K  \n",
        "**Topic:** Advanced LLM Architecture with Groq API  \n",
        "**Stack:** Python Â· Groq API Â· python-dotenv\n",
        "\n",
        "---\n",
        "\n",
        "## What I'm building\n",
        "\n",
        "I wanted to explore how production AI systems route queries to specialized sub-models, so I built a **Smart Customer Support Router** based on the Mixture of Experts (MoE) pattern.\n",
        "\n",
        "The idea is simple: instead of having one generic LLM handle everything, a lightweight **router** first classifies the user's intent, then forwards the query to the most appropriate **expert**:\n",
        "\n",
        "- ğŸ› ï¸ **Technical Expert** â€” handles bug reports and dev issues\n",
        "- ğŸ’³ **Billing Expert** â€” deals with payment and subscription problems\n",
        "- ğŸ›ï¸ **Sales Expert** â€” answers product and pricing questions\n",
        "- ğŸ“ˆ **Tool Use Expert** *(bonus)* â€” fetches real-time data\n",
        "- ğŸ’¬ **General Expert** â€” catches everything else\n",
        "\n",
        "Each expert has its own persona, system prompt, and temperature â€” tuned for its specific job."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-setup-header",
      "metadata": {
        "id": "cell-setup-header"
      },
      "source": [
        "## Step 1 â€” Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cell-install",
      "metadata": {
        "id": "cell-install"
      },
      "outputs": [],
      "source": [
        "%pip install groq python-dotenv --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-env-header",
      "metadata": {
        "id": "cell-env-header"
      },
      "source": [
        "## Step 2 â€” API Key Setup\n",
        "\n",
        "I keep my key in a `.env` file so I never accidentally commit it:\n",
        "\n",
        "```\n",
        "GROQ_API_KEY=your_groq_api_key_here\n",
        "```\n",
        "\n",
        "You can also uncomment the line below to set it inline (just don't push it to GitHub)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cell-env",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-env",
        "outputId": "f4d0a324-612b-4624-a9b8-fb5805fc0f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Groq client ready.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "# Using llama-3.3-70b-versatile â€” mixtral-8x7b is deprecated as of mid-2024\n",
        "BASE_MODEL = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "print(\"âœ… Groq client ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-config-header",
      "metadata": {
        "id": "cell-config-header"
      },
      "source": [
        "## Step 3 â€” Expert Configurations\n",
        "\n",
        "I define each expert as a dictionary entry with:\n",
        "- A **system prompt** that gives the LLM a specific persona and instructions\n",
        "- A **temperature** value (lower = more precise/deterministic, higher = more conversational)\n",
        "\n",
        "Getting the system prompts right took a few iterations â€” the billing one especially needed an empathetic tone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cell-config",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-config",
        "outputId": "aff5649e-8824-4554-f808-cf2d713b4be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 5 experts loaded: ['technical', 'billing', 'sales', 'tool_use', 'general']\n"
          ]
        }
      ],
      "source": [
        "EXPERT_CONFIG = {\n",
        "    \"technical\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are a Senior Software Engineer specializing in debugging and technical support. \"\n",
        "            \"Be precise and rigorous. Always identify the root cause before suggesting a fix. \"\n",
        "            \"Provide corrected code snippets with inline comments explaining each change. \"\n",
        "            \"Follow best practices and use accurate technical terminology. \"\n",
        "            \"Format all code in fenced code blocks with the appropriate language tag.\"\n",
        "        ),\n",
        "        \"temperature\": 0.3,\n",
        "    },\n",
        "    \"billing\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are a professional Billing and Accounts Specialist with a warm, reassuring tone. \"\n",
        "            \"Always start by acknowledging the customer's concern before jumping into solutions. \"\n",
        "            \"Walk them through refund procedures, subscription policies, and dispute resolution \"\n",
        "            \"clearly and step-by-step. Never expose sensitive financial data, and escalate \"\n",
        "            \"to a human agent whenever the situation warrants it.\"\n",
        "        ),\n",
        "        \"temperature\": 0.5,\n",
        "    },\n",
        "    \"sales\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are an enthusiastic Sales Consultant who genuinely wants to help customers \"\n",
        "            \"find the right plan or product for their needs. \"\n",
        "            \"Highlight relevant features and value clearly, mention current promotions, \"\n",
        "            \"and always end with a specific call-to-action. Be persuasive, not pushy.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7,\n",
        "    },\n",
        "    \"tool_use\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are a financial data assistant. Real-time data will be injected into your context. \"\n",
        "            \"Use it to answer the user's question accurately. \"\n",
        "            \"Always mention the data source and note that prices update frequently.\"\n",
        "        ),\n",
        "        \"temperature\": 0.2,\n",
        "    },\n",
        "    \"general\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are a friendly, helpful general-purpose assistant. \"\n",
        "            \"Keep your answers clear and conversational. \"\n",
        "            \"Be concise unless the user asks for a detailed explanation.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(f\"âœ… {len(EXPERT_CONFIG)} experts loaded: {list(EXPERT_CONFIG.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-tool-header",
      "metadata": {
        "id": "cell-tool-header"
      },
      "source": [
        "## Step 4 â€” Bonus: Mock Crypto Price Tool\n",
        "\n",
        "For the tool-use expert, I wrote a mock function that simulates a real-time price API.  \n",
        "In a production setup you'd swap this for a live call to CoinGecko or Binance â€” the function signature stays the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cell-tool",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-tool",
        "outputId": "a2f9c44b-5895-45ca-990c-779ec9dab1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'symbol': 'BTC', 'price_usd': 68420.5, 'change_24h': '+2.3%'}\n",
            "{'symbol': 'ETH', 'price_usd': 3512.75, 'change_24h': '-0.8%'}\n"
          ]
        }
      ],
      "source": [
        "def fetch_crypto_price(asset: str) -> dict:\n",
        "    \"\"\"\n",
        "    Simulates a real-time crypto price lookup.\n",
        "\n",
        "    In production, replace the mock_prices dict with a live API call, e.g.:\n",
        "        import requests\n",
        "        r = requests.get(f'https://api.coingecko.com/api/v3/simple/price?ids={asset}&vs_currencies=usd')\n",
        "        return r.json()\n",
        "\n",
        "    Args:\n",
        "        asset: Name or ticker of the cryptocurrency (case-insensitive)\n",
        "\n",
        "    Returns:\n",
        "        dict with symbol, price_usd, change_24h â€” or an error key if not found\n",
        "    \"\"\"\n",
        "    mock_prices = {\n",
        "        \"bitcoin\":  {\"symbol\": \"BTC\", \"price_usd\": 68_420.50, \"change_24h\": \"+2.3%\"},\n",
        "        \"ethereum\": {\"symbol\": \"ETH\", \"price_usd\":  3_512.75, \"change_24h\": \"-0.8%\"},\n",
        "        \"solana\":   {\"symbol\": \"SOL\", \"price_usd\":    172.40, \"change_24h\": \"+4.1%\"},\n",
        "    }\n",
        "\n",
        "    query = asset.lower()\n",
        "    for name, data in mock_prices.items():\n",
        "        if name in query or data[\"symbol\"].lower() in query:\n",
        "            return data\n",
        "\n",
        "    return {\"error\": f\"No price data available for '{asset}'\"}\n",
        "\n",
        "\n",
        "# Sanity check\n",
        "print(fetch_crypto_price(\"bitcoin\"))\n",
        "print(fetch_crypto_price(\"ETH\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-router-header",
      "metadata": {
        "id": "cell-router-header"
      },
      "source": [
        "## Step 5 â€” The Router: `classify_intent()`\n",
        "\n",
        "This is the core of the MoE pattern. I use a separate LLM call at `temperature=0` (fully deterministic)\n",
        "to classify the incoming query into one of the five categories.  \n",
        "Keeping `max_tokens=10` ensures we get just the category word â€” no extra fluff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cell-router",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-router",
        "outputId": "ba9ec467-9b50-4baf-ae4d-ec35e06b8f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "technical\n",
            "billing\n",
            "tool_use\n"
          ]
        }
      ],
      "source": [
        "def classify_intent(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    Routes a user query to the correct expert category.\n",
        "\n",
        "    Categories:\n",
        "        technical | billing | sales | tool_use | general\n",
        "\n",
        "    Returns a single lowercase string. Falls back to 'general' if the\n",
        "    model returns something unexpected.\n",
        "    \"\"\"\n",
        "    router_system_prompt = \"\"\"You are an intent classification engine for a multi-expert customer support system.\n",
        "Classify the user message into EXACTLY ONE of the following categories:\n",
        "\n",
        "- technical   : bug reports, code errors, crashes, API problems, developer how-to questions\n",
        "- billing     : charges, refunds, invoices, subscriptions, payment disputes\n",
        "- sales       : pricing inquiries, new product interest, plan upgrades, demo requests\n",
        "- tool_use    : requests for live/real-time data such as crypto prices, stocks, or weather\n",
        "- general     : greetings, small talk, or anything that doesn't fit the above\n",
        "\n",
        "Rules:\n",
        "1. Output ONLY the single category word in lowercase.\n",
        "2. No punctuation, no explanation, no extra text whatsoever.\n",
        "3. When in doubt, return: general\"\"\"\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=BASE_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": router_system_prompt},\n",
        "            {\"role\": \"user\",   \"content\": user_input},\n",
        "        ],\n",
        "        temperature=0,   # Deterministic â€” routing must be consistent\n",
        "        max_tokens=10,   # Category is a single word; cap tokens to avoid noise\n",
        "    )\n",
        "\n",
        "    category = resp.choices[0].message.content.strip().lower()\n",
        "\n",
        "    # Guard against unexpected outputs\n",
        "    if category not in EXPERT_CONFIG:\n",
        "        print(f\"  âš ï¸  Unexpected category '{category}' â€” defaulting to 'general'\")\n",
        "        category = \"general\"\n",
        "\n",
        "    return category\n",
        "\n",
        "\n",
        "# Quick tests\n",
        "print(classify_intent(\"My Python script is throwing an IndexError\"))\n",
        "print(classify_intent(\"I was charged twice this month\"))\n",
        "print(classify_intent(\"What is the price of Ethereum?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-orchestrator-header",
      "metadata": {
        "id": "cell-orchestrator-header"
      },
      "source": [
        "## Step 6 â€” The Orchestrator: `handle_request()`\n",
        "\n",
        "This function ties everything together:\n",
        "1. Calls `classify_intent()` to figure out which expert should respond\n",
        "2. If `tool_use`: extracts the asset name, calls the mock tool, and injects the result into the expert's context\n",
        "3. Otherwise: makes a standard LLM call using the appropriate expert's config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cell-orchestrator",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-orchestrator",
        "outputId": "d2ec5ff1-caa6-4e36-dc5f-ae70fafa3f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Orchestrator ready.\n"
          ]
        }
      ],
      "source": [
        "def handle_request(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    MoE orchestrator â€” routes a query and calls the appropriate expert.\n",
        "\n",
        "    Flow:\n",
        "        user_input â†’ classify_intent() â†’ category\n",
        "            if tool_use  â†’ fetch live data â†’ tool_use expert\n",
        "            else         â†’ relevant expert LLM call\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  ğŸ“¨ User Input  : {user_input}\")\n",
        "\n",
        "    # â”€â”€ 1. Classify â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    category = classify_intent(user_input)\n",
        "    print(f\"  ğŸ”€ Routed To   : [{category.upper()} EXPERT]\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # â”€â”€ 2a. Tool-use branch (bonus) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if category == \"tool_use\":\n",
        "        # Use a focused LLM call to pull out just the asset name\n",
        "        asset_resp = client.chat.completions.create(\n",
        "            model=BASE_MODEL,\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"Extract only the cryptocurrency or asset name from the query below. \"\n",
        "                    f\"Return ONLY the asset name (e.g., 'bitcoin', 'ethereum', 'solana'). \"\n",
        "                    f\"Query: '{user_input}'\"\n",
        "                )\n",
        "            }],\n",
        "            temperature=0,\n",
        "            max_tokens=10,\n",
        "        )\n",
        "        asset_name = asset_resp.choices[0].message.content.strip().lower()\n",
        "\n",
        "        # Fetch mock price data\n",
        "        price_data = fetch_crypto_price(asset_name)\n",
        "        print(f\"  ğŸ”§ Tool Result : fetch_crypto_price('{asset_name}') â†’ {price_data}\")\n",
        "\n",
        "        if \"error\" in price_data:\n",
        "            return f\"Sorry, I couldn't find price data for '{asset_name}'.\"\n",
        "\n",
        "        # Inject tool result into the expert's context\n",
        "        augmented_context = (\n",
        "            f\"Live market data for {price_data['symbol']}:\\n\"\n",
        "            f\"  Current Price : ${price_data['price_usd']:,.2f} USD\\n\"\n",
        "            f\"  24h Change    : {price_data['change_24h']}\\n\"\n",
        "            f\"  Data Source   : Mock Market Feed (simulated)\\n\\n\"\n",
        "            f\"User question: {user_input}\"\n",
        "        )\n",
        "        result = client.chat.completions.create(\n",
        "            model=BASE_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": EXPERT_CONFIG[\"tool_use\"][\"system_prompt\"]},\n",
        "                {\"role\": \"user\",   \"content\": augmented_context},\n",
        "            ],\n",
        "            temperature=EXPERT_CONFIG[\"tool_use\"][\"temperature\"],\n",
        "            max_tokens=300,\n",
        "        )\n",
        "        return result.choices[0].message.content\n",
        "\n",
        "    # â”€â”€ 2b. Standard expert branch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    expert = EXPERT_CONFIG[category]\n",
        "    result = client.chat.completions.create(\n",
        "        model=BASE_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": expert[\"system_prompt\"]},\n",
        "            {\"role\": \"user\",   \"content\": user_input},\n",
        "        ],\n",
        "        temperature=expert[\"temperature\"],\n",
        "        max_tokens=512,\n",
        "    )\n",
        "    return result.choices[0].message.content\n",
        "\n",
        "\n",
        "print(\"âœ… Orchestrator ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-demo-header",
      "metadata": {
        "id": "cell-demo-header"
      },
      "source": [
        "## Step 7 â€” End-to-End Demo\n",
        "\n",
        "Running all five routing paths to verify everything works together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cell-demo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cell-demo",
        "outputId": "0cc06b35-99f9-4ed7-aa74-23b11e236f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "  ğŸ“¨ User Input  : I'm getting a KeyError when accessing a dict inside a list of dicts.\n",
            "  ğŸ”€ Routed To   : [TECHNICAL EXPERT]\n",
            "============================================================\n",
            "\n",
            "ğŸ’¬ Response:\n",
            "To accurately diagnose and resolve the `KeyError`, we need to understand the structure of your data and the code that's causing the error. However, I can provide a general approach to troubleshooting and fixing this issue.\n",
            "\n",
            "### Understanding the Error\n",
            "A `KeyError` occurs when you try to access a key in a dictionary that does not exist. When dealing with a list of dictionaries, this could mean that the key you're trying to access is not present in one or more of the dictionaries in the list.\n",
            "\n",
            "### Example Scenario\n",
            "Let's consider an example where we have a list of dictionaries representing users, and each dictionary should have a 'name' and an 'age'. However, one of the dictionaries is missing the 'age' key.\n",
            "\n",
            "```python\n",
            "users = [\n",
            "    {'name': 'John', 'age': 30},\n",
            "    {'name': 'Jane'},  # Missing 'age' key\n",
            "    {'name': 'Bob', 'age': 40}\n",
            "]\n",
            "\n",
            "# Trying to access the 'age' key in each dictionary\n",
            "for user in users:\n",
            "    print(user['age'])  # This will raise a KeyError for Jane\n",
            "```\n",
            "\n",
            "### Identifying the Root Cause\n",
            "The root cause here is that not all dictionaries in the list contain the 'age' key. To fix this, we need to ensure that we handle the case where the key is missing.\n",
            "\n",
            "### Suggested Fix\n",
            "One way to handle missing keys is to use the `.get()` method of dictionaries, which allows you to specify a default value to return if the key is not found.\n",
            "\n",
            "```python\n",
            "users = [\n",
            "    {'name': 'John', 'age': 30},\n",
            "    {'name': 'Jane'},  # Missing 'age' key\n",
            "    {'name': 'Bob', 'age': 40}\n",
            "]\n",
            "\n",
            "# Using .get() to safely access the 'age' key\n",
            "for user in users:\n",
            "    age = user.get('age')  # Returns None if 'age' key is not found\n",
            "    if age is None:\n",
            "        print(f\"User {user['name']} is missing age information.\")\n",
            "    else:\n",
            "        print(f\"{user['name']} is {age} years old.\")\n",
            "```\n",
            "\n",
            "Alternatively, you could use a try-except block to catch and handle the `KeyError`:\n",
            "\n",
            "```python\n",
            "for user in users:\n",
            "    try:\n",
            "        print(user['age'])\n",
            "    except KeyError:\n",
            "        print(f\"User {user['name']} is missing age information.\")\n",
            "```\n",
            "\n",
            "### Best Practice\n",
            "Always validate the structure\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "  ğŸ“¨ User Input  : I got charged twice for my subscription last week.\n",
            "  ğŸ”€ Routed To   : [BILLING EXPERT]\n",
            "============================================================\n",
            "\n",
            "ğŸ’¬ Response:\n",
            "I'm so sorry to hear that you were charged twice for your subscription. I can understand how frustrating that must be for you. I'm here to help you resolve this issue as quickly and efficiently as possible.\n",
            "\n",
            "First, I want to assure you that we take situations like this very seriously and will do our best to rectify the error. Can you please tell me a little more about what happened? For example, what date did you notice the duplicate charge, and what method of payment was used?\n",
            "\n",
            "Also, I just want to let you know that I'll be happy to walk you through our refund procedure and ensure that you receive a full refund for the duplicate charge. If needed, I can also escalate this issue to a dedicated agent who can provide additional assistance. Your satisfaction is our top priority, and we'll work together to get this sorted out for you.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "  ğŸ“¨ User Input  : Can you tell me what's included in the Pro plan?\n",
            "  ğŸ”€ Routed To   : [SALES EXPERT]\n",
            "============================================================\n",
            "\n",
            "ğŸ’¬ Response:\n",
            "Our Pro plan is one of our most popular options, and for good reason. It's packed with features that can really help take your business or personal projects to the next level.\n",
            "\n",
            "With the Pro plan, you'll get access to advanced tools like priority customer support, so you can get help whenever you need it. You'll also get increased storage space, so you can keep all your files and data in one place. Plus, you'll get additional features like enhanced security and collaboration tools, making it easy to work with others and keep your information safe.\n",
            "\n",
            "One of the things that really sets our Pro plan apart is the ability to customize and personalize your experience. You'll get access to a range of templates and design tools, so you can create a look and feel that's all your own. And, with our integrations with popular third-party apps, you can connect all your favorite tools and services in one place.\n",
            "\n",
            "Right now, we're running a special promotion that can save you 10% on your first year's subscription to the Pro plan. This is a limited-time offer, so be sure to act soon to take advantage of the discount.\n",
            "\n",
            "If you're interested in learning more about the Pro plan and how it can help you achieve your goals, I'd be happy to set up a demo or answer any questions you may have. Shall we schedule a call to discuss further and see if the Pro plan is the right fit for you?\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "  ğŸ“¨ User Input  : What's the current price of Bitcoin?\n",
            "  ğŸ”€ Routed To   : [TOOL_USE EXPERT]\n",
            "============================================================\n",
            "  ğŸ”§ Tool Result : fetch_crypto_price('bitcoin') â†’ {'symbol': 'BTC', 'price_usd': 68420.5, 'change_24h': '+2.3%'}\n",
            "\n",
            "ğŸ’¬ Response:\n",
            "The current price of Bitcoin is $68,420.50 USD, according to the Mock Market Feed (simulated). Please note that prices update frequently, so this information may change rapidly.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "  ğŸ“¨ User Input  : Hey, what can you help me with?\n",
            "  ğŸ”€ Routed To   : [GENERAL EXPERT]\n",
            "============================================================\n",
            "\n",
            "ğŸ’¬ Response:\n",
            "I can help with all sorts of things. Need help with a question, a problem, or just looking for some information? I can assist with topics like history, science, language, and more. Or if you're feeling stuck, I can even help with generating ideas or brainstorming. What's on your mind?\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_queries = [\n",
        "    \"I'm getting a KeyError when accessing a dict inside a list of dicts.\",  # â†’ technical\n",
        "    \"I got charged twice for my subscription last week.\",                     # â†’ billing\n",
        "    \"Can you tell me what's included in the Pro plan?\",                       # â†’ sales\n",
        "    \"What's the current price of Bitcoin?\",                                   # â†’ tool_use (bonus)\n",
        "    \"Hey, what can you help me with?\",                                        # â†’ general\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    answer = handle_request(query)\n",
        "    print(f\"\\nğŸ’¬ Response:\\n{answer}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-interactive-header",
      "metadata": {
        "id": "cell-interactive-header"
      },
      "source": [
        "## Step 8 â€” Interactive Mode\n",
        "\n",
        "Type any query and watch the router decide which expert handles it. Type `exit` to quit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cell-interactive",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-interactive",
        "outputId": "4f84ba02-4d6b-4ea8-bff7-8dacd6cadf9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– MoE Customer Support Router â€” Interactive Mode\n",
            "Type 'exit' or 'quit' to stop.\n",
            "\n",
            "You: Sourabh  S Hegde PES2UG23CS929\n",
            "\n",
            "============================================================\n",
            "  ğŸ“¨ User Input  : Sourabh  S Hegde PES2UG23CS929\n",
            "  ğŸ”€ Routed To   : [GENERAL EXPERT]\n",
            "============================================================\n",
            "\n",
            "ğŸ’¬ Response:\n",
            "It seems like you've shared your name and a student ID, Sourabh. Is there something I can help you with or would you like to chat?\n",
            "\n",
            "------------------------------------------------------------\n",
            "You: What is the price of Solana?\n",
            "\n",
            "============================================================\n",
            "  ğŸ“¨ User Input  : What is the price of Solana?\n",
            "  ğŸ”€ Routed To   : [TOOL_USE EXPERT]\n",
            "============================================================\n",
            "  ğŸ”§ Tool Result : fetch_crypto_price('solana') â†’ {'symbol': 'SOL', 'price_usd': 172.4, 'change_24h': '+4.1%'}\n",
            "\n",
            "ğŸ’¬ Response:\n",
            "The current price of Solana (SOL) is $172.40 USD, according to the Mock Market Feed (simulated). Please note that prices update frequently, so this information may change rapidly.\n",
            "\n",
            "------------------------------------------------------------\n",
            "You: exit\n",
            "Session ended.\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ¤– MoE Customer Support Router â€” Interactive Mode\")\n",
        "print(\"Type 'exit' or 'quit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "    if not user_input or user_input.lower() in (\"exit\", \"quit\"):\n",
        "        print(\"Session ended.\")\n",
        "        break\n",
        "    response = handle_request(user_input)\n",
        "    print(f\"\\nğŸ’¬ Response:\\n{response}\\n\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-arch-header",
      "metadata": {
        "id": "cell-arch-header"
      },
      "source": [
        "## Architecture Overview\n",
        "\n",
        "```\n",
        "User Query\n",
        "    â”‚\n",
        "    â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   ROUTER  classify_intent()      â”‚  â† temperature=0, max_tokens=10\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                   â”‚ category\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚      ORCHESTRATOR  handle_request()          â”‚\n",
        "    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "       â”‚          â”‚          â”‚          â”‚          â”‚\n",
        "       â–¼          â–¼          â–¼          â–¼          â–¼\n",
        "    Tech       Billing     Sales    Tool Use   General\n",
        "   Expert      Expert     Expert    Expert     Expert\n",
        "```\n",
        "\n",
        "| Expert | Temperature | Purpose |\n",
        "|--------|-------------|--------|\n",
        "| Technical | 0.3 | Debugging, code review |\n",
        "| Billing | 0.5 | Refunds, payment issues |\n",
        "| Sales | 0.7 | Product info, upgrades |\n",
        "| Tool Use | 0.2 | Real-time market data |\n",
        "| General | 0.7 | Fallback / casual chat |\n",
        "\n",
        "### Key design choices\n",
        "- **Deterministic routing** (`temperature=0`) avoids flaky classification\n",
        "- **Narrow `max_tokens` on the router** prevents verbose or ambiguous outputs\n",
        "- **Fallback guard** in `classify_intent()` ensures unknown outputs never crash the pipeline\n",
        "- **Injected tool context** keeps the tool-use branch clean â€” the expert sees pre-fetched data, not a raw tool call"
      ]
    }
  ]
}